* Paper: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
* Link: https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf

Concept: CNNs are good for vision, transformers are good for NLP. Can we make transformers a backbone for vision ?

Problem of transformers that prevent its adaptability to images:
- Quadratic complexity, feasible on a text of thousands of words, more complex for images that can contain up to million of pixels
- Scale: Texts are embedded into token, with the 'full vocabulary' usually known beforehand. Images can have different scales, making the same element be represented multiple times in the same way (Circle, bigger circle).


SWINE: Builds hierarchical feature maps, obtains linear computational complexity to image size

- First layers act on dense very close pixels, further layers look into larger patches of images sequentially
- There is a rolling window that changes the patch's layout on the images, making the number of patches fixed but changing which patches are within windows at each step.

Curious: Swin does not use positional embeddings, but positional cues in the self-attention computation.

Dataset: Imagenet-1K; 1.28M training, 50k validation, 1k classes
