{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "unipen_root = \"../../data/handwriting/Unipen/train_r01_v07/include\"\n",
    "\n",
    "data_providers = os.listdir(unipen_root)\n",
    "\n",
    "for data_provider in data_providers:\n",
    "    print(f\"Looking at provider {data_provider}\")\n",
    "    data_provider_path = os.path.join(unipen_root, data_provider)\n",
    "    data_path = os.path.join(data_provider_path, \"data\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data provider {data_provider} without data\")\n",
    "        continue\n",
    "\n",
    "    doc_files = [filename for filename in os.listdir(data_provider_path) if filename != \"data\"]\n",
    "    print(f\"Doc files: {doc_files}\")\n",
    "    print(f\"Stroke files: {os.listdir(data_path)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class UnipenKeywords(Enum):\n",
    "    # Config-related\n",
    "    X_DIM = \"X_DIM\"\n",
    "    Y_DIM = \"Y_DIM\"\n",
    "    X_POINTS_PER_INCH = \"X_POINTS_PER_INCH\"\n",
    "    Y_POINTS_PER_INCH = \"Y_POINTS_PER_INCH\"\n",
    "    X_POINTS_PER_MM = \"X_POINTS_PER_MM\"\n",
    "    Y_POINTS_PER_MM = \"Y_POINTS_PER_MM\"\n",
    "\n",
    "    POINTS_PER_SECOND = \"POINTS_PER_SECOND\"\n",
    "    COORD = \"COORD\"\n",
    "\n",
    "    # Stroke-related\n",
    "    PEN_UP = \"PEN_UP\"\n",
    "    PEN_DOWN = \"PEN_DOWN\"\n",
    "    START_BOX = \"START_BOX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class UnipenHandler():\n",
    "    handler_root: str\n",
    "    strokes: list[list[int, int, bool]]\n",
    "\n",
    "    COMMAND_PATTERN = r\"^\\.(\\w*) ?(.*)$\"\n",
    "\n",
    "    def __init__(self, handler_root: str):\n",
    "        self.handler_root = handler_root\n",
    "        self.strokes = []\n",
    "\n",
    "    def process_doc(self):\n",
    "        \"\"\"Process the documentation of this Unipen handler, preparing helpers to decipher the stroke data files.\n",
    "        The specific handler prepare its config there\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config_for_datafile(self, datafile_path: str) -> dict:\n",
    "        \"\"\"Get the configuration for the given data file. Specific to handler type.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def search_values_in_file(self, filepath: str, search_keys: set[str], throw_on_missing: bool) -> dict:\n",
    "        \"\"\"Helper function.\n",
    "        Search the given values inside of the file. Return every match in a dictionnary\n",
    "        The function will either return empty values for missing keys, or throw an exception\"\"\"\n",
    "        result_dict = {}\n",
    "        found_keys = set()\n",
    "        \n",
    "        with open(filepath, \"rt\") as f:\n",
    "\n",
    "            for line in f.readlines():\n",
    "                command_match = re.match(UnipenHandler.COMMAND_PATTERN, line)\n",
    "                if command_match:\n",
    "                    if command_match.group(1) in search_keys:\n",
    "                        key = command_match.group(1)\n",
    "                        values = command_match.group(2).strip()\n",
    "                        if \" \" in values:\n",
    "                            values = values.split(\" \")\n",
    "                        found_keys.add(key)\n",
    "                        try:\n",
    "                            result_dict[key].extend(values)\n",
    "                        except KeyError:\n",
    "                            result_dict[key] = values if isinstance(values, list) else [values]\n",
    "\n",
    "                #Early termination: \n",
    "                set_diff = search_keys-found_keys\n",
    "                if len(set_diff) == 0:\n",
    "                    break\n",
    "        \n",
    "        set_diff = search_keys-found_keys\n",
    "        if len(set_diff) > 0:\n",
    "            error = f\"Missing key values {set_diff} on file {filepath}\"\n",
    "            if throw_on_missing:\n",
    "                raise Exception(error)\n",
    "            else:\n",
    "                print(error)\n",
    "        \n",
    "        return result_dict\n",
    "\n",
    "    def create_strokes(self): \n",
    "        \"\"\"In single doc format, we can open the 'data' folder and expect files to be strokes / folder of files\"\"\"\n",
    "        data_folder = os.path.join(self.handler_root, \"data\")\n",
    "        self.scan_data_folder(data_folder)\n",
    "    \n",
    "    def scan_data_folder(self, data_folder: str):\n",
    "        \"\"\"Scan a data folder in search of stroke files. In case of nested folder, scan nested folders\"\"\"\n",
    "        print(f\"Scanning {data_folder}\")\n",
    "        for filename in os.listdir(data_folder):\n",
    "            filepath = os.path.join(data_folder, filename)\n",
    "            if os.path.isdir(filepath):\n",
    "                self.scan_data_folder(filepath)\n",
    "            elif os.path.isfile(filepath):\n",
    "                strokes = self.read_stroke_file(filepath)\n",
    "                self.strokes.extend(strokes)\n",
    "\n",
    "    def end_stroke(self, stroke: list, config: dict) -> list:\n",
    "        \"\"\"End a stroke and register if it is not empty.\n",
    "        Return an empty stroke: Either the empty stroke in input or a new empty array\"\"\"\n",
    "        if len(stroke) > 0:\n",
    "            processed_stroke = self.process_stroke(np.array(stroke), config)\n",
    "            self.strokes.append(processed_stroke)\n",
    "            return []\n",
    "        else:\n",
    "            return stroke\n",
    "\n",
    "    def read_stroke_file(self, filepath: str) -> list[tuple[int, int, bool]]:\n",
    "        \"\"\"Read a stroke file and return the list as a (x, y, penUp) signal\"\"\"\n",
    "        configuration = self.get_config_for_datafile(filepath)\n",
    "        coord_config: list = configuration[UnipenKeywords.COORD.value]\n",
    "\n",
    "        idx, idy = coord_config.index(\"X\"), coord_config.index(\"Y\")\n",
    "\n",
    "        #Read the line iteratively, line by line. Check if line is an instruction (.INSTRUCTION). If not, try to retrieve coordinates.\n",
    "        strokes = []\n",
    "        with open(filepath, \"rt\") as f:\n",
    "            #Start at the first PEN instruction\n",
    "            is_started = False\n",
    "            pen_down = False\n",
    "            current_stroke = []\n",
    "            for line in f:\n",
    "                line = line.rstrip().strip()\n",
    "                \n",
    "                #Gap\n",
    "                if is_started and not line:\n",
    "                    current_stroke = self.end_stroke(current_stroke, configuration)\n",
    "                    continue\n",
    "\n",
    "                command_match = re.match(UnipenHandler.COMMAND_PATTERN, line)\n",
    "                if command_match:\n",
    "                    match command_match.group(1):\n",
    "                        #the penup signal is encoded in the last value of the stroke\n",
    "                        case UnipenKeywords.PEN_UP.value:\n",
    "                            is_started = True\n",
    "                            pen_down = False\n",
    "                            if len(current_stroke) > 0:\n",
    "                                current_stroke[-1][2] = True\n",
    "                        case UnipenKeywords.PEN_DOWN.value:\n",
    "                            is_started = True\n",
    "                            pen_down = True\n",
    "                            #Well, some providers (anj) put a pen down WITHOUT a pen up, so we have to add the penup signal anyway.\n",
    "                            if len(current_stroke) > 0:\n",
    "                                current_stroke[-1][2] = True\n",
    "                        case _:\n",
    "                            continue\n",
    "                else:\n",
    "                    if not is_started or line.startswith(\"#\"):\n",
    "                        continue\n",
    "                    \n",
    "                    # Not empty line and not command: Assume coordinate line.\n",
    "                    coords = line.strip().split()\n",
    "\n",
    "                    x, y = int(coords[idx]), int(coords[idy])\n",
    "                    if x==0 and y==0:\n",
    "                        current_stroke = self.end_stroke(current_stroke, configuration)\n",
    "                    else:\n",
    "                        # Register points if and only if pen is down.\n",
    "                        if pen_down:\n",
    "                            current_stroke.append([x, y, False])\n",
    "\n",
    "        #Add last current stroke\n",
    "        self.end_stroke(current_stroke, configuration)\n",
    "\n",
    "        return strokes\n",
    "\n",
    "    def resample_by_interpolation(self, signal, input_fs, output_fs):\n",
    "        \"\"\"Re-sampling using scypi linear interpolation\n",
    "        credit to: https://stackoverflow.com/questions/51420923/resampling-a-signal-with-scipy-signal-resample\n",
    "        Additional credits\n",
    "        DISCLAIMER: This function is copied from https://github.com/nwhitehead/swmixer/blob/master/swmixer.py, \n",
    "        which was released under LGPL. \n",
    "        \"\"\"\n",
    "\n",
    "        scale = output_fs / input_fs\n",
    "        # calculate new length of sample. Keep minimum of 1 point\n",
    "        n = max(round(len(signal) * scale), 1)\n",
    "\n",
    "        # use linear interpolation\n",
    "        # endpoint keyword means than linspace doesn't go all the way to 1.0\n",
    "        # If it did, there are some off-by-one errors\n",
    "        # e.g. scale=2.0, [1,2,3] should go to [1,1.5,2,2.5,3,3]\n",
    "        # but with endpoint=True, we get [1,1.4,1.8,2.2,2.6,3]\n",
    "        # Both are OK, but since resampling will often involve\n",
    "        # exact ratios (i.e. for 44100 to 22050 or vice versa)\n",
    "        # using endpoint=False gets less noise in the resampled sound\n",
    "        resampled_signal = np.interp(\n",
    "            np.linspace(0.0, 1.0, n, endpoint=False),  # where to interpret\n",
    "            np.linspace(0.0, 1.0, len(signal), endpoint=False),  # known positions\n",
    "            signal,  # known data points\n",
    "        )\n",
    "        return resampled_signal\n",
    "\n",
    "    def resample_stroke(self, stroke: np.ndarray, in_scale: int, out_scale: int) -> np.ndarray:\n",
    "        \"\"\"Resample a stroke by resampling every sub-stroke, thus respecting the penup signals\"\"\"\n",
    "\n",
    "        #Cut the signal into sub-signal separated by the penup instruction\n",
    "        substrokes = []\n",
    "        current_substroke = []\n",
    "\n",
    "        for (x, y, penup) in stroke:\n",
    "            current_substroke.append([x, y])\n",
    "            if penup:\n",
    "                substrokes.append(np.array(current_substroke))\n",
    "                current_substroke = []\n",
    "\n",
    "        if len(current_substroke) > 0:\n",
    "            substrokes.append(np.array(current_substroke))\n",
    "            \n",
    "        #Resample x,y values\n",
    "        resampled_substrokes = []\n",
    "        for substroke in substrokes:\n",
    "            print(f\"Substroke: {substroke.shape}, in: {in_scale}, out: {out_scale}\")\n",
    "            x_resampled = np.rint(self.resample_by_interpolation(substroke[:, 0], in_scale, out_scale)).astype(int)\n",
    "            y_resampled = np.rint(self.resample_by_interpolation(substroke[:, 1], in_scale, out_scale)).astype(int)\n",
    "            print(f\"x_resample size: {x_resampled.shape}\")\n",
    "            penup_resampled = np.full_like(x_resampled, fill_value=False)\n",
    "            penup_resampled[-1] = True\n",
    "\n",
    "            resampled_substroke = np.concatenate(\n",
    "                (x_resampled[:, np.newaxis], y_resampled[:, np.newaxis], penup_resampled[:, np.newaxis]), \n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            resampled_substrokes.append(resampled_substroke)\n",
    "\n",
    "        #Re-construct the stroke\n",
    "        final_stroke = np.concatenate(resampled_substrokes, axis=0, dtype=np.int32)\n",
    "        return final_stroke\n",
    "\n",
    "    def process_stroke(self, stroke: list[tuple[int, int, bool]], config: dict):\n",
    "        \"\"\"Apply post processing to the strokes\n",
    "        Config: Configuration of the stroke, especially containing spatial info (DPI, PPI, etc) and spatial info (Points per seconds)\"\"\"\n",
    "        align_strokes = True\n",
    "        start_padding = 5\n",
    "\n",
    "        #Y axis is inversed between plotting and signal\n",
    "        max_y = max(stroke[:, 1])\n",
    "        stroke[:, 1] = max_y - stroke[:, 1]\n",
    "\n",
    "        #Alignement / padding\n",
    "        if align_strokes:\n",
    "            min_x = min(stroke[:, 0])\n",
    "            stroke[:, 0] -= (min_x - start_padding)\n",
    "\n",
    "            min_y = min(stroke[:, 1])\n",
    "            stroke[:, 1] -= (min_y - start_padding)\n",
    "\n",
    "        # Frequency adjustment\n",
    "        signal_freq = int(config[UnipenKeywords.POINTS_PER_SECOND.value][0])\n",
    "        target_pps = 100 #100PPS, or 10MS\n",
    "        if signal_freq != target_pps:\n",
    "            stroke = self.resample_stroke(stroke, signal_freq, target_pps)\n",
    "\n",
    "        #TODO\n",
    "        #Pix distance adjustment\n",
    "        # X_POINTS_PER_INCH = \"X_POINTS_PER_INCH\"\n",
    "        # Y_POINTS_PER_INCH = \"Y_POINTS_PER_INCH\"\n",
    "        # X_POINTS_PER_MM = \"X_POINTS_PER_MM\"\n",
    "        # Y_POINTS_PER_MM = \"Y_POINTS_PER_MM\"\n",
    "\n",
    "        return stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SingleDocFileHandler(UnipenHandler):\n",
    "    \"\"\"This handler manages Unipen provider with a single data file and a data/ folder containing strokes.\n",
    "    If a or multiple lex file exist, ignore them\"\"\"\n",
    "    config: dict\n",
    "\n",
    "    def __init__(self, handler_root: str):\n",
    "        super().__init__(handler_root)\n",
    "\n",
    "        self.process_doc()\n",
    "\n",
    "    def process_doc(self):\n",
    "        \"\"\"Process the documentation of this Unipen handler, preparing helpers to decipher the stroke data files\"\"\"\n",
    "        #Retrieve single doc file\n",
    "        files = os.listdir(self.handler_root)\n",
    "        doc_candidates = [file for file in files if file.endswith(\".doc\")]\n",
    "        if len(doc_candidates) != 1:\n",
    "            raise Exception(f\"Provider {self.handler_root}: Expected one doc file, found {doc_candidates}\")\n",
    "\n",
    "        doc_file = doc_candidates[0]\n",
    "\n",
    "        #Parse doc file in order to find the data required to interpret the stroke\n",
    "        required_search_values = set([UnipenKeywords.POINTS_PER_SECOND.value, UnipenKeywords.COORD.value])\n",
    "        opt_search_values = set([\n",
    "            UnipenKeywords.X_DIM.value, UnipenKeywords.Y_DIM.value,\n",
    "            UnipenKeywords.X_POINTS_PER_INCH.value, UnipenKeywords.Y_POINTS_PER_INCH.value, \n",
    "            UnipenKeywords.X_POINTS_PER_MM.value, UnipenKeywords.Y_POINTS_PER_MM.value, \n",
    "        ])\n",
    "\n",
    "        doc_filepath = os.path.join(self.handler_root, doc_file)\n",
    "        self.config = {}\n",
    "        self.config.update(self.search_values_in_file(doc_filepath, required_search_values, throw_on_missing=True))\n",
    "        self.config.update(self.search_values_in_file(doc_filepath, opt_search_values, throw_on_missing=False))\n",
    "        \n",
    "\n",
    "    def get_config_for_datafile(self, datafile_path: str) -> dict:\n",
    "        \"\"\"Simple: In single doc mode the configuration is always the same\"\"\"\n",
    "        return self.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class MultiDocFileHandler(UnipenHandler):\n",
    "    \"\"\"This handler manages Unipen provider with multiple doc files and a data/ folder containing strokes.\n",
    "    It parses every config file and assumes that the data folder will be related to the doc files.\n",
    "    If a or multiple lex file exist, ignore them\"\"\"\n",
    "    configs: dict\n",
    "    \n",
    "    def __init__(self, handler_root: str):\n",
    "        super().__init__(handler_root)\n",
    "\n",
    "        self.process_doc()\n",
    "\n",
    "    def process_doc(self):\n",
    "        \"\"\"Process the documentation of this Unipen handler, preparing helpers to decipher the stroke data files\"\"\"\n",
    "        files = os.listdir(self.handler_root)\n",
    "        doc_files = [file for file in files if file.endswith(\".doc\")]\n",
    "\n",
    "        self.configs = {}\n",
    "\n",
    "        for doc_file in doc_files:\n",
    "            #Parse doc file in order to find the data required to interpret the stroke\n",
    "            required_search_values = set([UnipenKeywords.POINTS_PER_SECOND.value, UnipenKeywords.COORD.value])\n",
    "            opt_search_values = set([\n",
    "                UnipenKeywords.X_DIM.value, UnipenKeywords.Y_DIM.value,\n",
    "                UnipenKeywords.X_POINTS_PER_INCH.value, UnipenKeywords.Y_POINTS_PER_INCH.value, \n",
    "                UnipenKeywords.X_POINTS_PER_MM.value, UnipenKeywords.Y_POINTS_PER_MM.value, \n",
    "            ])\n",
    "\n",
    "            doc_filepath = os.path.join(self.handler_root, doc_file)\n",
    "\n",
    "            config_file = {}\n",
    "            \n",
    "            config_file.update(self.search_values_in_file(doc_filepath, required_search_values, throw_on_missing=True))\n",
    "            config_file.update(self.search_values_in_file(doc_filepath, opt_search_values, throw_on_missing=False))\n",
    "            doc_file_key = Path(doc_file).stem\n",
    "            # some providers have header_{folder}.doc files, it is very easy to remove the header part\n",
    "            if \"header_\" in doc_file_key:\n",
    "                doc_file_key = doc_file_key.removeprefix(\"header_\")\n",
    "            print(f\"Create config {doc_file_key}\")\n",
    "            self.configs[doc_file_key] = config_file\n",
    "\n",
    "    def get_config_for_datafile(self, datafile_path: str) -> dict:\n",
    "        \"\"\"Return the doc file corresponding to the data folder of the data file.\"\"\"\n",
    "        data_path = Path(datafile_path)\n",
    "        data_folder = data_path.parent.name\n",
    "        try:\n",
    "            print(f\"Requesting config for {datafile_path}, searching key {data_folder}\")\n",
    "            return self.configs[data_folder]\n",
    "        except KeyError:\n",
    "            doc_name = data_path.stem\n",
    "            #For TOS: TOS does not consider the folder to have the same name as the doc file, but a 1;1 datafile - docfile\n",
    "            print(f\"Requesting config for {datafile_path}, searching key {doc_name}\")\n",
    "            return self.configs[doc_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class NoDocFileHandler(UnipenHandler):\n",
    "    \"\"\"This handler manages Unipen provider with no doc files and a data/ folder containing strokes.\n",
    "    In this configuration, it is assumed that each data file must contain the documentation at its start. As the search in file function returns\n",
    "    when values are found, it means minimal overhead (2IO operations instead of 1)\n",
    "    If a or multiple lex file exist, ignore them\"\"\"\n",
    "    configs: dict\n",
    "    \n",
    "    def __init__(self, handler_root: str):\n",
    "        super().__init__(handler_root)\n",
    "\n",
    "        self.process_doc()\n",
    "\n",
    "    def process_doc(self):\n",
    "        \"\"\"Process the documentation of this Unipen handler, assuming the data files are the documentation\"\"\"\n",
    "        data_path = os.path.join(self.handler_root, \"data\")\n",
    "        files = os.listdir(data_path)\n",
    "\n",
    "        self.configs = {}\n",
    "\n",
    "        for file in files:\n",
    "            #Parse doc file in order to find the data required to interpret the stroke\n",
    "            required_search_values = set([UnipenKeywords.POINTS_PER_SECOND.value, UnipenKeywords.COORD.value])\n",
    "            opt_search_values = set([\n",
    "                UnipenKeywords.X_DIM.value, UnipenKeywords.Y_DIM.value,\n",
    "                UnipenKeywords.X_POINTS_PER_INCH.value, UnipenKeywords.Y_POINTS_PER_INCH.value, \n",
    "                UnipenKeywords.X_POINTS_PER_MM.value, UnipenKeywords.Y_POINTS_PER_MM.value, \n",
    "            ])\n",
    "\n",
    "            doc_filepath = os.path.join(data_path, file)\n",
    "            if os.path.isdir(doc_filepath):\n",
    "                child_files = [os.path.join(file, child_file) for child_file in os.listdir(doc_filepath)]\n",
    "                files.extend(child_files)\n",
    "                continue\n",
    "\n",
    "            config_file = {}\n",
    "            \n",
    "            config_file.update(self.search_values_in_file(doc_filepath, required_search_values, throw_on_missing=True))\n",
    "            config_file.update(self.search_values_in_file(doc_filepath, opt_search_values, throw_on_missing=False))\n",
    "            doc_file_key = Path(doc_filepath).stem\n",
    "            # some providers have header_{folder}.doc files, it is very easy to remove the header part\n",
    "            if \"header_\" in doc_file_key:\n",
    "                doc_file_key = doc_file_key.removeprefix(\"header_\")\n",
    "            print(f\"Create config {doc_file_key}\")\n",
    "            self.configs[doc_file_key] = config_file\n",
    "\n",
    "    def get_config_for_datafile(self, datafile_path: str) -> dict:\n",
    "        \"\"\"Return the doc file corresponding to the data folder of the data file.\"\"\"\n",
    "        doc_file_key = Path(datafile_path).stem\n",
    "        print(f\"Requesting config for {datafile_path}, searching key {doc_file_key}\")\n",
    "        return self.configs[doc_file_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class UnipenHandlerBuilder():\n",
    "    \"\"\"This class holds the knowledge of the UNIPEN data providers and their different formats.\n",
    "    It can be used to build the different online signals through builders adapted to providers.\"\"\"\n",
    "    unipen_root: str\n",
    "\n",
    "    provider_type_mapping = {\n",
    "        SingleDocFileHandler: [\n",
    "                                \"abm\", \"anj\", \"apa\", \"apb\", \"apc\", \"apd\", \"ape\", \"app\", \"att\", \n",
    "                               \"bba\", \"bbb\", \"bbc\", \"bbd\", \n",
    "                               \"cea\", \"ceb\", \"cec\", \"ced\", \"cee\",\n",
    "                               \"dar\", \"gmd\", \"imt\", \"int\", \n",
    "                               \"lex\", \"par\", \"pcl\", \"pri\",\n",
    "                               \"rim\", \"scr\", \"uqb\"\n",
    "                            ],\n",
    "\n",
    "        # MultiDocFileHandler: [\n",
    "        #                         \"hpb\", \"hpp\", \"huj\", \"tos\",\n",
    "        #                         \"kai\", \"kar\", \"lav\", \"lou\", \n",
    "        #                         \"mot\", \"pap\", \"phi\", \"sta\", \n",
    "        #                         \"syn\", \"val\", \"ugi\"\n",
    "        #                     ],\n",
    "                            \n",
    "        # NoDocFileHandler: [\n",
    "        #                         \"art\", \"aga\", \"atu\", \"nic\", \n",
    "        #                         \"sie\"\n",
    "        #                     ],\n",
    "        # Problems\n",
    "        #   Empty: ata, cef, ibm, imp\n",
    "        #   No temporal info: not\n",
    "\n",
    "        #Par: .inc file changed to .doc and COORD added to it, as every stroke file has the same COORD system. Avoids creating a single handler for this one.\n",
    "        #PCL: Rename internal_pad.doc to .doc.other as it has the same information as the pcl.doc file.\n",
    "        #phi: Renaned *file* to *file*.doc\n",
    "        #HPP: rename hpp doc files form hpb* into hpp*\n",
    "        #HUJ: Put manudo datafile into huj8/ folder in order to have corresponding doc file\n",
    "        #STA: rename hpb{0,1}.doc to sta{0,1}.doc\n",
    "        #Val: Separate two writers into val01, val02 folders to correspond to expected structure\n",
    "    }\n",
    "\n",
    "    def __init__(self, unipen_root: str) -> None:\n",
    "        self.unipen_root = unipen_root\n",
    "\n",
    "    def build_handlers(self) -> list[UnipenHandler]:\n",
    "        \"\"\"Build all the handlers declared by this provider maping\"\"\"\n",
    "        provider_return: list[UnipenHandler] = []\n",
    "        \n",
    "        for handler_class, providers in self.provider_type_mapping.items():\n",
    "            print(f\"Len of providers: {len(providers)}\")\n",
    "            for provider in providers:\n",
    "                try:\n",
    "                    print(f\"Building provider {provider} as {handler_class.__name__}\")\n",
    "                    provider_path = os.path.join(self.unipen_root, provider)\n",
    "                    provider_handler = handler_class(provider_path)\n",
    "                    provider_return.append(provider_handler)\n",
    "                except Exception as e:\n",
    "                    print(f\"Impossible to build provider {provider} due to error: {e}\")\n",
    "                    raise e\n",
    "\n",
    "        return provider_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unipen_root = \"../../data/handwriting/Unipen/train_r01_v07/include\"\n",
    "\n",
    "unipen_handler_builder = UnipenHandlerBuilder(unipen_root)\n",
    "handlers = unipen_handler_builder.build_handlers()\n",
    "\n",
    "print(f\"built {len(handlers)} handlers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_lengths = []\n",
    "for handler in handlers:\n",
    "    handler.create_strokes()\n",
    "    stroke_lengths.append(len(handler.strokes))\n",
    "    \n",
    "print(f\"Total strokes: {sum(stroke_lengths)}, repartition: {stroke_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DRAW_COLOR_BLACK = 255\n",
    "DRAW_COLOR_WHITE = 0\n",
    "\n",
    "def create_image(signal: list[int, int, bool], draw_color = 1):\n",
    "    \"\"\"Create the image associated with the given signal.\"\"\"\n",
    "    max_h =  int(math.ceil(max(signal[:, 0])))\n",
    "    max_w = int(math.ceil(max(signal[:, 1])))\n",
    "\n",
    "    canvas = np.ascontiguousarray(np.full((max_w + 2, max_h + 2), DRAW_COLOR_BLACK), dtype=np.uint8)\n",
    "    print(f\"Canvas: {canvas.shape}\")\n",
    "    \n",
    "    #Draw lines from point (t-1) to current point (t) IFF the pen was not up. start with penup\n",
    "    #as we start from point 0.\n",
    "    draw_current_stroke = False\n",
    "    for x, y, eos in signal:\n",
    "        if draw_current_stroke:\n",
    "            cv2.line(canvas, (last_x, last_y), (x, y), DRAW_COLOR_WHITE, draw_color) \n",
    "        last_x, last_y, draw_current_stroke = x, y, not eos\n",
    "    \n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reasonable_stroke_size(stroke: np.ndarray):\n",
    "    return 1 + int(max(stroke[:, 1]) / 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "for i, handler in enumerate(handlers):\n",
    "    print(f\"Handler {i}: {handler.handler_root}\")\n",
    "    strokes = handler.strokes\n",
    "\n",
    "    n = randint(0, len(strokes) - 3)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3)\n",
    "\n",
    "    for i, stroke in enumerate(strokes[n:n+3]):\n",
    "        print(stroke.shape)\n",
    "        \n",
    "        image = create_image(stroke, compute_reasonable_stroke_size(stroke))\n",
    "\n",
    "        axs[i].imshow(image, cmap='gray', vmin=0, vmax=255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing problematic providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unipen_root = \"../../data/handwriting/Unipen/train_r01_v07/include\"\n",
    "provider = \"ced\"\n",
    "\n",
    "handler = SingleDocFileHandler(os.path.join(unipen_root, provider))\n",
    "handler.create_strokes()\n",
    "print(f\"Strokes: {len(handler.strokes)}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "strokes = handler.strokes\n",
    "\n",
    "n = randint(0, len(strokes) - 4)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "\n",
    "for i, stroke in enumerate(strokes[n:n+3]):\n",
    "    print(stroke.shape)\n",
    "    image = create_image(stroke, compute_reasonable_stroke_size(stroke))\n",
    "\n",
    "    axs[i].imshow(image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
