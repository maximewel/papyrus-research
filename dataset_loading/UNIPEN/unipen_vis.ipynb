{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at provider abm\n",
      "Doc files: ['abm0.doc']\n",
      "Stroke files: ['ca0.dat', 'ibo0.dat', 'sm0.dat', 'wz0.dat']\n",
      "Looking at provider aga\n",
      "Doc files: []\n",
      "Stroke files: ['amywldrp.dat', 'annzchrl.dat', 'crlmtchl.dat', 'dawnmrsh.dat', 'drdvlllb.dat', 'elsbthey.dat', 'greghunt.dat', 'jamesmor.dat', 'jhnshrly.dat', 'lzbthwht.dat', 'mlssdrry.dat', 'nikkshmn.dat', 'scttjyns.dat', 'shrncrtr.dat']\n",
      "Looking at provider anj\n",
      "Doc files: ['anj.doc', 'anj.lex']\n",
      "Stroke files: ['g9303.dat', 'g9304.dat', 'g9312.dat', 'g9313.dat', 'g9314.dat', 'g9318.dat']\n",
      "Looking at provider apa\n",
      "Doc files: ['app.doc']\n",
      "Stroke files: ['apa00', 'apa01', 'apa02', 'apa03', 'apa04', 'apa09', 'apa10', 'apa12', 'apa13', 'apa14', 'apa15', 'apa16', 'apa19', 'apa20']\n",
      "Looking at provider apb\n",
      "Doc files: ['app.doc']\n",
      "Stroke files: ['apb02', 'apb04', 'apb05', 'apb06', 'apb07', 'apb08', 'apb09', 'apb10', 'apb12', 'apb13', 'apb14', 'apb15', 'apb16', 'apb17', 'apb18', 'apb20', 'apb21', 'apb22', 'apb23', 'apb24', 'apb25', 'apb27', 'apb28', 'apb29', 'apb31', 'apb32', 'apb33', 'apb34', 'apb35', 'apb36', 'apb38', 'apb39', 'apb40', 'apb41', 'apb42']\n",
      "Looking at provider apc\n",
      "Doc files: ['app.doc', 'app.doc.bad', 'app.doc~']\n",
      "Stroke files: ['apc17', 'apc18', 'apc19', 'apc20', 'apc21', 'apc22', 'apc23', 'apc24', 'apc25']\n",
      "Looking at provider apd\n",
      "Doc files: ['app.doc', 'app.doc.bad', 'app.doc~']\n",
      "Stroke files: ['apd03', 'apd04', 'apd06', 'apd07', 'apd08', 'apd09', 'apd11', 'apd13', 'apd14', 'apd15', 'apd16', 'apd17', 'apd22', 'apd24', 'apd25', 'apd26', 'apd28', 'apd29', 'apd30', 'apd33', 'apd34', 'apd37', 'apd38', 'apd39', 'apd40', 'apd41']\n",
      "Looking at provider ape\n",
      "Doc files: ['app.doc', 'app.doc.bad']\n",
      "Stroke files: ['ape01', 'ape02', 'ape03', 'ape04', 'ape05', 'ape06', 'ape07', 'ape08', 'ape09', 'ape10', 'ape11', 'ape16', 'ape17', 'ape18']\n",
      "Looking at provider app\n",
      "Doc files: ['app.doc']\n",
      "Stroke files: ['app05', 'app06', 'app07', 'app08', 'app09', 'app11', 'app12', 'app13', 'app14', 'app18', 'app19', 'app25', 'app26', 'app31', 'app32', 'app33', 'app34', 'app35', 'app36', 'app37', 'app38', 'app40', 'app41']\n",
      "Looking at provider art\n",
      "Doc files: []\n",
      "Stroke files: ['art01trn.dat', 'art01tst.dat', 'art03trn.dat', 'art03tst.dat', 'art08trn.dat', 'art08tst.dat']\n",
      "Looking at provider ata\n",
      "Data provider ata without data\n",
      "Looking at provider att\n",
      "Doc files: ['att0.doc', 'ccode.lex']\n",
      "Stroke files: ['att0']\n",
      "Looking at provider atu\n",
      "Doc files: []\n",
      "Stroke files: ['atu1', 'atu2']\n",
      "Looking at provider bba\n",
      "Doc files: ['bbn.lex', 'bbnE.doc']\n",
      "Stroke files: ['ajcr', 'aksr', 'dwcr', 'mmsr', 'wccl']\n",
      "Looking at provider bbb\n",
      "Doc files: ['bbn.lex', 'bbnD1.doc']\n",
      "Stroke files: ['ctwr', 'jmhr', 'mcjr']\n",
      "Looking at provider bbc\n",
      "Doc files: ['bbn.lex', 'bbnD2.doc']\n",
      "Stroke files: ['mecr', 'pshr', 'rxlr']\n",
      "Looking at provider bbd\n",
      "Doc files: ['bbn.lex', 'bbnT.doc']\n",
      "Stroke files: ['dsfr', 'dskl', 'hlgr', 'jabr', 'jwsr', 'sinr', 'wcdr']\n",
      "Looking at provider cea\n",
      "Doc files: ['ced0.doc', 'ced0.lex']\n",
      "Stroke files: ['Asundi.dat', 'Balgi.dat', 'Kaeslingk.dat', 'Parthasarathy.dat', 'Ramaswamy.dat', 'Toepfer.dat']\n",
      "Looking at provider ceb\n",
      "Doc files: ['ced0.lex', 'ced1.doc']\n",
      "Stroke files: ['Balgi.dat', 'Bouzy.dat', 'Butsch.dat', 'Cheng.dat']\n",
      "Looking at provider cec\n",
      "Doc files: ['ced0.lex', 'ced2.doc']\n",
      "Stroke files: ['Agrawal.dat', 'Asundi.dat', 'Balgi.dat', 'Barsottelli.dat', 'Carnevale.dat', 'Guin.dat', 'Guin_K.dat', 'Hamilton_B.dat', 'Healy.dat', 'Hu.dat', 'Kabalkin.dat', 'Kaeslingk.dat', 'Kilinskas.dat', 'Kleinberg_B.dat', 'Koontz.dat', 'Laderman.dat', 'Larson.dat', 'Limpert.dat', 'Michalak.dat', 'Ng.dat', 'Parthasarathy.dat', 'Patel.dat', 'Poslinski.dat', 'Prabhakar.dat', 'Ramaswamy.dat', 'Rembas.dat', 'Roy.dat', 'Satsangi.dat', 'Schwartz.dat', 'Song.dat', 'Talukdar.dat', 'Taylor.dat', 'Toepfer.dat', 'Vallone.dat', 'Wu.dat']\n",
      "Looking at provider ced\n",
      "Doc files: ['ced3.doc', 'ced3.lex']\n",
      "Stroke files: ['aaaa9018_s_2h.dat', 'aike2911_s_1z.dat', 'ayml3525_s_0k.dat', 'bngm-227_s_i5.dat', 'bykg6164_s_23.dat', 'cada7032_s_ie.dat', 'cypl9614_s_jd.dat', 'ddjh4871_s_03.dat', 'flad4341_s_jh.dat', 'gyen4328_s_2c.dat', 'hrts4205_s_i8.dat', 'jban4624_s_1h.dat', 'jekn5827_s_26.dat', 'jika4257_s_0o.dat', 'jnpk7049_s_1r.dat', 'jtkr3293_s_04.dat', 'kkfg6658_s_iq.dat', 'knds0342_s_13.dat', 'knke4574.a_s_jo.dat', 'maba4240_s_ir.dat', 'mlmt5536_s_27.dat', 'mwbk4038_s_1l.dat', 'nank4257_s_12.dat', 'napl4256_s_0f.dat', 'naty7589.a_s_2e.dat', 'nnwe7032_s_0m.dat', 'pnlg5032_s_19.dat', 'qale4877_s_ik.dat', 'racy0978_s_0l.dat', 'repy0371_s_1a.dat', 'rgzu9771_s_jn.dat', 'rigi0287_s_jc.dat', 'rish6239_s_2d.dat', 'risn4711_s_1i.dat', 'rmnn2357_s_j4.dat', 'shmh-105_s_1u.dat', 'sibi4900_s_j3.dat', 'sifg9828_s_11.dat', 'sncn4237.b_s_ib.dat', 'snsn5548_s_j5.dat', 'wekh3976_s_j0.dat', 'wmas2465_s_is.dat', 'ylsn6164_s_jm.dat']\n",
      "Looking at provider cee\n",
      "Doc files: ['ced4.doc', 'ced4.lex']\n",
      "Stroke files: ['aaaa9018_g_1z.dat', 'aike2911_g_1h.dat', 'ayml3525_g_0c.dat', 'flad4341_g_j4.dat', 'gyen4328_g_1t.dat', 'hrts4205_g_i4.dat', 'jekn5827_g_1n.dat', 'jika4257_g_0e.dat', 'kkfg6658_g_ii.dat', 'knds0342_g_0s.dat', 'knke4574.a_g_j7.dat', 'maba4240_g_ij.dat', 'mlmt5536_g_1o.dat', 'mwbk4038_g_1c.dat', 'nank4257_g_0r.dat', 'napl4256_g_07.dat', 'pnlg5032_g_0y.dat', 'qale4877_g_ic.dat', 'repy0371_g_0z.dat', 'rgzu9771_g_j6.dat', 'rigi0287_g_j3.dat', 'rish6239_g_1u.dat', 'risn4711_g_1a.dat', 'rmnn2357_g_is.dat', 'sibi4900_g_ir.dat', 'sifg9828_g_0q.dat', 'sncn4237.b_g_i6.dat', 'snsn5548_g_it.dat', 'wmas2465_g_ik.dat']\n",
      "Looking at provider cef\n",
      "Data provider cef without data\n",
      "Looking at provider dar\n",
      "Doc files: ['dar2.doc', 'iacocca.lex']\n",
      "Stroke files: ['dar2']\n",
      "Looking at provider gmd\n",
      "Doc files: ['gmd.doc']\n",
      "Stroke files: ['alek.dat', 'kast.dat', 'monu.dat']\n",
      "Looking at provider hpb\n",
      "Doc files: ['basic.lex', 'hpb0.doc', 'hpb1.doc', 'hpb2.doc', 'hpb2lex.lex', 'hpb3.doc', 'hpb3lex.lex', 'hpb4.doc', 'hpb4lex.lex', 'hpb5.doc', 'hpb5lex.lex', 'unipen.def']\n",
      "Stroke files: ['hpb4', 'hpb5']\n",
      "Looking at provider hpp\n",
      "Doc files: ['basic.lex', 'hpb2lex.lex', 'hpb3lex.lex', 'hpb4lex.lex', 'hpb5lex.lex', 'hpp0.doc', 'hpp1.doc', 'hpp2.doc', 'hpp3.doc', 'hpp4.doc', 'phpb5.doc', 'unipen.def']\n",
      "Stroke files: ['hpp2', 'hpp3']\n",
      "Looking at provider huj\n",
      "Doc files: ['huj0.doc', 'huj1.doc', 'huj2.doc', 'huj3.doc', 'huj4.doc', 'huj5.doc', 'huj6.doc', 'huj7.doc', 'huj8.doc']\n",
      "Stroke files: ['huj8']\n",
      "Looking at provider ibm\n",
      "Doc files: ['ibm0.doc', 'ibm1.doc', 'word.lex']\n",
      "Stroke files: ['ibm0', 'ibm1']\n",
      "Looking at provider imp\n",
      "Doc files: ['imp0.doc', 'imp1.doc']\n",
      "Stroke files: ['imp0', 'imp1']\n",
      "Looking at provider imt\n",
      "Doc files: ['imt.doc', 'imt.lex']\n",
      "Stroke files: ['imt0']\n",
      "Looking at provider int\n",
      "Doc files: ['int.doc']\n",
      "Stroke files: ['c.dat', 'e.dat', 'k.dat', 'l.dat']\n",
      "Looking at provider kai\n",
      "Doc files: ['kai0.doc', 'kai1.doc', 'kai2.doc', 'word.lex']\n",
      "Stroke files: ['kai0', 'kai1', 'kai2']\n",
      "Looking at provider kar\n",
      "Doc files: ['kar0.doc', 'kar0.lex', 'kar1.doc', 'kar1.lex', 'kar2.doc']\n",
      "Stroke files: ['kar1', 'kar2']\n",
      "Looking at provider lav\n",
      "Doc files: ['lav0.doc', 'lav1.doc', 'lav2.doc', 'lav3.doc', 'word.lex']\n",
      "Stroke files: ['lav0', 'lav1', 'lav2', 'lav3']\n",
      "Looking at provider lex\n",
      "Doc files: ['lex0.doc', 'lex0.freq']\n",
      "Stroke files: ['lex0']\n",
      "Looking at provider lou\n",
      "Doc files: ['lou0.doc', 'lou0.lex', 'lou1.doc', 'lou1.lex']\n",
      "Stroke files: ['lou0', 'lou1']\n",
      "Looking at provider mot\n",
      "Doc files: ['mot1.doc', 'mot2.doc']\n",
      "Stroke files: ['mot1']\n",
      "Looking at provider nic\n",
      "Doc files: []\n",
      "Stroke files: ['anton', 'corrie', 'eelco', 'hannie', 'johan', 'kees', 'mark', 'miep', 'saskia', 'tineke', 'willem']\n",
      "Looking at provider not\n",
      "Doc files: ['not0.doc', 'not1.doc']\n",
      "Stroke files: ['not0', 'not1']\n",
      "Looking at provider pap\n",
      "Doc files: ['pap0.doc', 'pap1.doc', 'words22k.lex']\n",
      "Stroke files: ['pap0', 'pap1']\n",
      "Looking at provider par\n",
      "Doc files: ['ink_data.doc']\n",
      "Stroke files: ['19mccl.dat', 'dgalli1.dat', 'dgalli2.dat', 'jmcnie1.dat', 'jmcnie2.dat', 'lani1.dat', 'lani2.dat', 'lynne.dat']\n",
      "Looking at provider pcl\n",
      "Doc files: ['pcl.doc', 'pcl_internal_pad.doc.other']\n",
      "Stroke files: ['eng17.dat', 'eng3.dat', 'eng4.dat', 'eng6.dat', 'engl2.dat', 'gleb.dat', 'irland1.dat', 'irs5.dat', 'kucha.dat', 'kuty-02.dat', 'kuty-04.dat', 'kuty-08.dat', 'me.dat', 'medved.dat', 'sasa.dat', 'sason1.dat', 'sason2.dat', 'sason3.dat', 'unknown1.dat', 'unknown2.dat', 'vlad.dat']\n",
      "Looking at provider phi\n",
      "Doc files: ['header_phi0.doc', 'header_phi1.doc', 'header_phi2.doc']\n",
      "Stroke files: ['phi0', 'phi1', 'phi2']\n",
      "Looking at provider pri\n",
      "Doc files: ['pri.doc']\n",
      "Stroke files: ['pri0', 'pri1', 'pri2', 'pri3']\n",
      "Looking at provider rim\n",
      "Doc files: ['rimon.doc']\n",
      "Stroke files: ['wd136.dat', 'wd137.dat', 'wd143.dat', 'wd144.dat', 'wd145.dat', 'wd146.dat', 'wd150.dat', 'wd161.dat', 'wd167.dat', 'wd172.dat', 'wd175.dat', 'wd177.dat', 'wd180.dat', 'wd183.dat', 'wd190.dat', 'wd193.dat', 'wd206.dat', 'wd209.dat', 'wd210.dat', 'wd216.dat', 'wd217.dat']\n",
      "Looking at provider scr\n",
      "Doc files: ['scr.doc']\n",
      "Stroke files: ['d1', 'd2', 'd3']\n",
      "Looking at provider sie\n",
      "Doc files: []\n",
      "Stroke files: ['char', 'word']\n",
      "Looking at provider sta\n",
      "Doc files: ['basic.lex', 'hpb2.doc', 'hpb2lex.lex', 'hpb3.doc', 'hpb3lex.lex', 'hpb4.doc', 'hpb4lex.lex', 'hpb5.doc', 'hpb5lex.lex', 'sta0.doc', 'sta1.doc', 'unipen.def']\n",
      "Stroke files: ['sta0', 'sta1']\n",
      "Looking at provider syn\n",
      "Doc files: ['syn0.doc', 'syn1.doc']\n",
      "Stroke files: ['syn0', 'syn1']\n",
      "Looking at provider tos\n",
      "Doc files: ['f1001060.doc', 'f1002058.doc', 'f1005066.doc', 'f1006061.doc', 'f1007061.doc', 'f1008058.doc', 'f1009062.doc', 'f1011062.doc', 'f1012064.doc', 'f1013066.doc', 'f1014063.doc', 'f1015066.doc', 'f1016058.doc', 'f1017062.doc', 'f1018060.doc', 'f1020059.doc', 'f1021064.doc', 'f1023064.doc', 'f1024058.doc', 'f1025063.doc', 'f1026058.doc', 'f1027064.doc', 'f1029060.doc', 'f1030059.doc', 'f1032061.doc', 'f1034059.doc', 'f1035060.doc', 'f1036063.doc', 'f1037061.doc', 'f1038060.doc', 'f1039061.doc', 'f1040063.doc', 'f1041060.doc', 'f1042059.doc', 'f1043061.doc', 'f1044066.doc', 'f1045065.doc', 'f1046061.doc', 'f1048063.doc', 'f1049057.doc', 'f1050060.doc', 'f1051061.doc', 'f1052063.doc', 'f1053060.doc', 'f1054063.doc', 'f1055065.doc', 'f1056061.doc', 'f1057065.doc', 'f1058059.doc', 'f1059062.doc', 'f1060061.doc', 'f1061063.doc', 'f1062065.doc', 'f1063064.doc', 'f1065059.doc', 'f1066065.doc', 'f1067066.doc', 'f1069065.doc', 'f1070065.doc', 'f1071060.doc', 'f1078063.doc', 'f1079060.doc', 'f1080062.doc', 'f1081063.doc', 'f1100059.doc', 'f2001065.doc', 'f2002064.doc', 'f2004062.doc', 'f2005061.doc', 'f2006055.doc', 'f2007060.doc', 'f2009066.doc', 'f2010060.doc', 'f2011063.doc', 'f2012062.doc', 'f2013062.doc', 'f2014061.doc', 'f2015066.doc', 'f2016063.doc', 'f2017063.doc', 'f2018063.doc', 'f2021064.doc', 'f2022060.doc', 'f2023063.doc', 'f2024065.doc', 'f2025065.doc', 'f2027062.doc', 'f2028059.doc', 'f2029060.doc', 'f2030066.doc', 'f2031060.doc', 'f2032060.doc', 'f2033066.doc', 'f2034065.doc', 'f2035059.doc', 'f2036065.doc', 'f2037065.doc', 'f2038066.doc', 'f2040066.doc', 'f2041064.doc', 'f2042062.doc', 'f2043062.doc', 'f2044065.doc', 'f2045064.doc', 'f2046061.doc', 'f2047062.doc', 'f2048065.doc', 'f2049060.doc', 'f2051059.doc', 'f2052061.doc', 'f2053061.doc', 'f2054065.doc', 'f2055062.doc', 'f2056060.doc', 'f2057059.doc', 'f2059065.doc', 'f2061066.doc', 'f2062063.doc', 'f2063059.doc', 'f2064063.doc', 'f2065066.doc', 'f2066062.doc', 'f2067064.doc', 'f2068065.doc', 'f2100060.doc', 'f3001062.doc', 'f3002064.doc', 'f3005058.doc', 'f3006059.doc', 'f3007066.doc', 'f3009060.doc', 'f3010058.doc', 'f3011063.doc', 'f3012066.doc', 'f3013062.doc', 'f3014059.doc', 'f3015065.doc', 'f3016066.doc', 'f3017065.doc', 'f3018065.doc', 'f3020066.doc', 'f3021061.doc', 'f3022066.doc', 'f3023064.doc', 'f3024061.doc', 'f3025059.doc', 'f3026061.doc', 'f3027059.doc', 'f3028062.doc', 'f3029060.doc', 'f3030058.doc', 'f3031065.doc', 'f3032065.doc', 'f3033062.doc', 'f3034064.doc', 'f3036063.doc', 'f3037061.doc', 'f3038061.doc', 'f3040063.doc', 'f3042066.doc', 'f3043063.doc', 'f3045059.doc', 'f3046066.doc', 'f3047059.doc', 'f3048064.doc', 'f3049065.doc', 'f3050060.doc', 'f3051060.doc', 'f3052059.doc', 'f3053066.doc', 'f3054059.doc', 'f3055062.doc', 'f3057060.doc', 'f3058066.doc', 'f3059063.doc', 'f3060061.doc', 'f3061062.doc', 'f3062062.doc', 'f3064063.doc', 'f3065058.doc', 'f3066058.doc', 'f3067059.doc', 'f3068063.doc', 'f3070060.doc', 'f3071063.doc', 'f3072063.doc', 'f3073065.doc', 't1001065.doc', 't1002062.doc', 't1003062.doc', 't1004063.doc', 't1005063.doc', 't1006065.doc', 't1007061.doc', 't1008065.doc', 't1009064.doc', 't1010066.doc', 't1011057.doc', 't1012066.doc', 't1013062.doc', 't1014065.doc', 't1015063.doc', 't1016060.doc', 't1017065.doc', 't1018062.doc', 't1020062.doc', 't1021066.doc', 't1022066.doc', 't1024061.doc', 't1025066.doc', 't1026062.doc', 't1027062.doc', 't1029065.doc', 't1030059.doc', 't1032062.doc', 't1034064.doc', 't1035063.doc', 't1036060.doc', 't1037064.doc', 't1038065.doc', 't1039062.doc', 't1041065.doc', 't1042062.doc', 't1043065.doc', 't1044063.doc', 't1045066.doc', 't1046059.doc', 't1049061.doc', 't1050062.doc', 't1051062.doc', 't1052060.doc', 't1053065.doc', 't1054062.doc', 't1055066.doc', 't1057066.doc', 't1058064.doc', 't1059065.doc', 't1060063.doc', 't1061065.doc', 't1062061.doc', 't1063065.doc', 't1065064.doc', 't1066065.doc', 't1068065.doc', 't1069062.doc', 't1070059.doc', 't1071066.doc', 't1078062.doc', 't1079061.doc', 't1080066.doc', 't1081062.doc', 't1100060.doc', 't2001064.doc', 't2002062.doc', 't2003063.doc', 't2004060.doc', 't2005061.doc', 't2006065.doc', 't2007065.doc', 't2009066.doc', 't2010061.doc', 't2012066.doc', 't2013063.doc', 't2014063.doc', 't2015060.doc', 't2016065.doc', 't2017063.doc', 't2018065.doc', 't2019063.doc', 't2021063.doc', 't2022063.doc', 't2023065.doc', 't2024062.doc', 't2025064.doc', 't2027062.doc', 't2028063.doc', 't2029066.doc', 't2030058.doc', 't2031066.doc', 't2032064.doc', 't2033065.doc', 't2035063.doc', 't2036062.doc', 't2037064.doc', 't2038062.doc', 't2040060.doc', 't2042066.doc', 't2043060.doc', 't2044064.doc', 't2045060.doc', 't2046064.doc', 't2047064.doc', 't2048063.doc', 't2049062.doc', 't2051065.doc', 't2052057.doc', 't2053063.doc', 't2054060.doc', 't2055065.doc', 't2056062.doc', 't2057060.doc', 't2058064.doc', 't2059059.doc', 't2061062.doc', 't2062064.doc', 't2063063.doc', 't2064064.doc', 't2065065.doc', 't2066065.doc', 't2067066.doc', 't2068064.doc', 't3001065.doc', 't3002066.doc', 't3005064.doc', 't3006062.doc', 't3007062.doc', 't3009064.doc', 't3010064.doc', 't3011062.doc', 't3012065.doc', 't3014060.doc', 't3016062.doc', 't3018066.doc', 't3020063.doc', 't3021066.doc', 't3022060.doc', 't3024066.doc', 't3026064.doc', 't3027063.doc', 't3029064.doc', 't3030063.doc', 't3031061.doc', 't3033065.doc', 't3034061.doc', 't3036066.doc', 't3037060.doc', 't3038064.doc', 't3040066.doc', 't3042061.doc', 't3043061.doc', 't3045062.doc', 't3046064.doc', 't3047062.doc', 't3048065.doc', 't3049063.doc', 't3050062.doc', 't3052065.doc', 't3053061.doc', 't3054062.doc', 't3057063.doc', 't3058059.doc', 't3059061.doc', 't3060065.doc', 't3061065.doc', 't3062066.doc', 't3063065.doc', 't3064066.doc', 't3065062.doc', 't3066061.doc', 't3067063.doc', 't3068059.doc', 't3071064.doc', 't3072063.doc', 't3073065.doc']\n",
      "Stroke files: ['f1', 'f2', 'f3', 't1', 't2', 't3']\n",
      "Looking at provider ugi\n",
      "Doc files: ['alby.doc', 'andrea.doc', 'dehmanci.doc', 'gino.doc', 'jj.doc', 'otto.doc', 'piero.doc', 'sandra.doc', 'silvia.doc', 'ugi.lex', 'vittorio.doc']\n",
      "Stroke files: ['ugi0']\n",
      "Looking at provider uqb\n",
      "Doc files: ['uqb0.doc']\n",
      "Stroke files: ['ja.dat', 'pe.dat', 'py.dat', 're.dat']\n",
      "Looking at provider val\n",
      "Doc files: ['val01.doc', 'val02.doc']\n",
      "Stroke files: ['val01', 'val02']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "unipen_root = \"../../data/handwriting/Unipen/train_r01_v07/include\"\n",
    "\n",
    "data_providers = os.listdir(unipen_root)\n",
    "\n",
    "for data_provider in data_providers:\n",
    "    print(f\"Looking at provider {data_provider}\")\n",
    "    data_provider_path = os.path.join(unipen_root, data_provider)\n",
    "    data_path = os.path.join(data_provider_path, \"data\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data provider {data_provider} without data\")\n",
    "        continue\n",
    "\n",
    "    doc_files = [filename for filename in os.listdir(data_provider_path) if filename != \"data\"]\n",
    "    print(f\"Doc files: {doc_files}\")\n",
    "    print(f\"Stroke files: {os.listdir(data_path)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class UnipenKeywords(Enum):\n",
    "    # Config-related\n",
    "    X_DIM = \"X_DIM\"\n",
    "    Y_DIM = \"Y_DIM\"\n",
    "    X_POINTS_PER_INCH = \"X_POINTS_PER_INCH\"\n",
    "    Y_POINTS_PER_INCH = \"Y_POINTS_PER_INCH\"\n",
    "    X_POINTS_PER_MM = \"X_POINTS_PER_MM\"\n",
    "    Y_POINTS_PER_MM = \"Y_POINTS_PER_MM\"\n",
    "\n",
    "    POINTS_PER_SECOND = \"POINTS_PER_SECOND\"\n",
    "    COORD = \"COORD\"\n",
    "\n",
    "    # Stroke-related\n",
    "    PEN_UP = \"PEN_UP\"\n",
    "    PEN_DOWN = \"PEN_DOWN\"\n",
    "    START_BOX = \"START_BOX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class UnipenHandler():\n",
    "    handler_root: str\n",
    "    strokes: list[list[int, int, bool]]\n",
    "\n",
    "    COMMAND_PATTERN = r\"^\\.(\\w*) ?(.*)$\"\n",
    "\n",
    "    def __init__(self, handler_root: str):\n",
    "        self.handler_root = handler_root\n",
    "        self.strokes = []\n",
    "\n",
    "    def process_doc(self):\n",
    "        \"\"\"Process the documentation of this Unipen handler, preparing helpers to decipher the stroke data files.\n",
    "        The specific handler prepare its config there\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config_for_datafile(self, datafile_path: str) -> dict:\n",
    "        \"\"\"Get the configuration for the given data file. Specific to handler type.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def search_values_in_file(self, filepath: str, search_keys: set[str], throw_on_missing: bool) -> dict:\n",
    "        \"\"\"Helper function.\n",
    "        Search the given values inside of the file. Return every match in a dictionnary\n",
    "        The function will either return empty values for missing keys, or throw an exception\"\"\"\n",
    "        result_dict = {}\n",
    "        found_keys = set()\n",
    "        \n",
    "        with open(filepath, \"rt\") as f:\n",
    "\n",
    "            for line in f.readlines():\n",
    "                command_match = re.match(UnipenHandler.COMMAND_PATTERN, line)\n",
    "                if command_match:\n",
    "                    if command_match.group(1) in search_keys:\n",
    "                        key = command_match.group(1)\n",
    "                        values = command_match.group(2).strip()\n",
    "                        if \" \" in values:\n",
    "                            values = values.split(\" \")\n",
    "                        found_keys.add(key)\n",
    "                        try:\n",
    "                            result_dict[key].extend(values)\n",
    "                        except KeyError:\n",
    "                            result_dict[key] = [*values]\n",
    "        \n",
    "        set_diff = search_keys-found_keys\n",
    "        if len(set_diff) > 0:\n",
    "            error = f\"Missing key values {set_diff} on file {filepath}\"\n",
    "            if throw_on_missing:\n",
    "                raise Exception(error)\n",
    "            else:\n",
    "                print(error)\n",
    "        \n",
    "        return result_dict\n",
    "\n",
    "    def create_strokes(self): \n",
    "        \"\"\"In single doc format, we can open the 'data' folder and expect files to be strokes / folder of files\"\"\"\n",
    "        data_folder = os.path.join(self.handler_root, \"data\")\n",
    "        self.scan_data_folder(data_folder)\n",
    "    \n",
    "    def scan_data_folder(self, data_folder: str):\n",
    "        \"\"\"Scan a data folder in search of stroke files. In case of nested folder, scan nested folders\"\"\"\n",
    "        print(f\"Scanning {data_folder}\")\n",
    "        for filename in os.listdir(data_folder):\n",
    "            filepath = os.path.join(data_folder, filename)\n",
    "            if os.path.isdir(filepath):\n",
    "                self.scan_data_folder(filepath)\n",
    "            elif os.path.isfile(filepath):\n",
    "                strokes = self.read_stroke_file(filepath)\n",
    "                self.strokes.extend(strokes)\n",
    "\n",
    "    def end_stroke(self, stroke: list, config: dict):\n",
    "        \"\"\"End a stroke and register it\"\"\"\n",
    "        if len(stroke) > 0:\n",
    "            processed_stroke = self.process_stroke(np.array(stroke), config)\n",
    "            self.strokes.append(processed_stroke)\n",
    "\n",
    "    def read_stroke_file(self, filepath: str) -> list[tuple[int, int, bool]]:\n",
    "        \"\"\"Read a stroke file and return the list as a (x, y, penUp) signal\"\"\"\n",
    "        configuration = self.get_config_for_datafile(filepath)\n",
    "        coord_config: list = configuration[UnipenKeywords.COORD.value]\n",
    "\n",
    "        idx, idy = coord_config.index(\"X\"), coord_config.index(\"Y\")\n",
    "\n",
    "        #Read the line iteratively, line by line. Check if line is an instruction (.INSTRUCTION). If not, try to retrieve coordinates.\n",
    "        strokes = []\n",
    "        with open(filepath, \"rt\") as f:\n",
    "            #Start at the first PEN instruction\n",
    "            is_started = False\n",
    "            pen_down = False\n",
    "            current_stroke = []\n",
    "            for line in f:\n",
    "                line = line.rstrip()\n",
    "\n",
    "                #Gap\n",
    "                if is_started and not line:\n",
    "                    self.end_stroke(current_stroke, configuration)\n",
    "                    current_stroke = []\n",
    "                    continue\n",
    "\n",
    "                command_match = re.match(UnipenHandler.COMMAND_PATTERN, line)\n",
    "                if command_match:\n",
    "                    match command_match.group(1):\n",
    "                        #the penup signal is encoded in the last value of the stroke\n",
    "                        case UnipenKeywords.PEN_UP.value:\n",
    "                            is_started = True\n",
    "                            pen_down = False\n",
    "                            if len(current_stroke) > 0:\n",
    "                                current_stroke[-1][2] = True\n",
    "                        case UnipenKeywords.PEN_DOWN.value:\n",
    "                            is_started = True\n",
    "                            pen_down = True\n",
    "                            #Well, some providers (anj) put a pen down WITHOUT a pen up, so we have to add the penup signal anyway.\n",
    "                            if len(current_stroke) > 0:\n",
    "                                current_stroke[-1][2] = True\n",
    "                        case _:\n",
    "                            continue\n",
    "                else:\n",
    "                    if not is_started:\n",
    "                        continue\n",
    "                    \n",
    "                    # Not empty line and not command: Assume coordinate line.\n",
    "                    coords = line.strip().split()\n",
    "\n",
    "                    x, y = int(coords[idx]), int(coords[idy])\n",
    "                    if x==0 and y==0:\n",
    "                        self.end_stroke(current_stroke, configuration)\n",
    "                        current_stroke = []\n",
    "                    else:\n",
    "                        # Register points if and only if pen is down.\n",
    "                        if pen_down:\n",
    "                            current_stroke.append([x, y, False])\n",
    "\n",
    "        #Add last current stroke\n",
    "        self.end_stroke(current_stroke, configuration)\n",
    "\n",
    "        return strokes\n",
    "    \n",
    "    def process_stroke(self, stroke: list[tuple[int, int, bool]], config: dict):\n",
    "        \"\"\"Apply post processing to the strokes\n",
    "        Config: Configuration of the stroke, especially containing spatial info (DPI, PPI, etc) and spatial info (Points per seconds)\"\"\"\n",
    "        align_strokes = True\n",
    "        start_padding = 2\n",
    "\n",
    "        #Y axis is inversed between plotting and signal\n",
    "        max_y = max(stroke[:, 1])\n",
    "        stroke[:, 1] = max_y - stroke[:, 1]\n",
    "\n",
    "        #Alignement / padding\n",
    "        if align_strokes:\n",
    "            min_x = min(stroke[:, 0])\n",
    "            stroke[:, 0] -= (min_x - start_padding)\n",
    "\n",
    "            min_y = min(stroke[:, 1])\n",
    "            stroke[:, 1] -= (min_y - start_padding)\n",
    "\n",
    "        #TODO\n",
    "        # Frequency adjustment\n",
    "\n",
    "        #TODO\n",
    "        #Pix distance adjustment\n",
    "\n",
    "        return stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SingleDocFileHandler(UnipenHandler):\n",
    "    \"\"\"This handler manages Unipen provider with a single data file and a data/ folder containing strokes.\n",
    "    If a or multiple lex file exist, ignore them\"\"\"\n",
    "    config: dict\n",
    "\n",
    "    \n",
    "    def __init__(self, handler_root: str):\n",
    "        super().__init__(handler_root)\n",
    "\n",
    "        self.process_doc()\n",
    "\n",
    "    def process_doc(self):\n",
    "        \"\"\"Process the documentation of this Unipen handler, preparing helpers to decipher the stroke data files\"\"\"\n",
    "        #Retrieve single doc file\n",
    "        files = os.listdir(self.handler_root)\n",
    "        doc_candidates = [file for file in files if file.endswith(\".doc\")]\n",
    "        if len(doc_candidates) != 1:\n",
    "            raise Exception(f\"Provider {self.handler_root}: Expected one doc file, found {doc_candidates}\")\n",
    "\n",
    "        doc_file = doc_candidates[0]\n",
    "\n",
    "        #Parse doc file in order to find the data required to interpret the stroke\n",
    "        required_search_values = set([UnipenKeywords.POINTS_PER_SECOND.value, UnipenKeywords.COORD.value])\n",
    "        opt_search_values = set([\n",
    "            UnipenKeywords.X_DIM.value, UnipenKeywords.Y_DIM.value,\n",
    "            UnipenKeywords.X_POINTS_PER_INCH.value, UnipenKeywords.Y_POINTS_PER_INCH.value, \n",
    "            UnipenKeywords.X_POINTS_PER_MM.value, UnipenKeywords.Y_POINTS_PER_MM.value, \n",
    "        ])\n",
    "\n",
    "        doc_filepath = os.path.join(self.handler_root, doc_file)\n",
    "        self.config = {}\n",
    "        self.config.update(self.search_values_in_file(doc_filepath, required_search_values, throw_on_missing=True))\n",
    "        self.config.update(self.search_values_in_file(doc_filepath, opt_search_values, throw_on_missing=False))\n",
    "        \n",
    "\n",
    "    def get_config_for_datafile(self, datafile_path: str) -> dict:\n",
    "        \"\"\"Simple: In single doc mode the configuration is always the same\"\"\"\n",
    "        return self.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class MultiDocFileHandler(UnipenHandler):\n",
    "    \"\"\"This handler manages Unipen provider with multiple data files and a data/ folder containing strokes.\n",
    "    It parses every config file and assumes that the data folder will be related to the doc files.\n",
    "    If a or multiple lex file exist, ignore them\"\"\"\n",
    "    configs: dict\n",
    "    \n",
    "    def __init__(self, handler_root: str):\n",
    "        super().__init__(handler_root)\n",
    "\n",
    "        self.process_doc()\n",
    "\n",
    "    def process_doc(self):\n",
    "        \"\"\"Process the documentation of this Unipen handler, preparing helpers to decipher the stroke data files\"\"\"\n",
    "        files = os.listdir(self.handler_root)\n",
    "        doc_files = [file for file in files if file.endswith(\".doc\")]\n",
    "\n",
    "        self.configs = {}\n",
    "\n",
    "        for doc_file in doc_files:\n",
    "            #Parse doc file in order to find the data required to interpret the stroke\n",
    "            required_search_values = set([UnipenKeywords.POINTS_PER_SECOND.value, UnipenKeywords.COORD.value])\n",
    "            opt_search_values = set([\n",
    "                UnipenKeywords.X_DIM.value, UnipenKeywords.Y_DIM.value,\n",
    "                UnipenKeywords.X_POINTS_PER_INCH.value, UnipenKeywords.Y_POINTS_PER_INCH.value, \n",
    "                UnipenKeywords.X_POINTS_PER_MM.value, UnipenKeywords.Y_POINTS_PER_MM.value, \n",
    "            ])\n",
    "\n",
    "            doc_filepath = os.path.join(self.handler_root, doc_file)\n",
    "\n",
    "            config_file = {}\n",
    "            \n",
    "            config_file.update(self.search_values_in_file(doc_filepath, required_search_values, throw_on_missing=True))\n",
    "            config_file.update(self.search_values_in_file(doc_filepath, opt_search_values, throw_on_missing=False))\n",
    "            doc_file_key = Path(doc_file).stem\n",
    "            # some providers have header_{folder}.doc files, it is very easy to remove the header part\n",
    "            if \"header_\" in doc_file_key:\n",
    "                doc_file_key = doc_file_key.removeprefix(\"header_\")\n",
    "            print(f\"Create config {doc_file_key}\")\n",
    "            self.configs[doc_file_key] = config_file\n",
    "\n",
    "    def get_config_for_datafile(self, datafile_path: str) -> dict:\n",
    "        \"\"\"Return the doc file corresponding to the data folder of the data file.\"\"\"\n",
    "        from pathlib import Path\n",
    "        data_folder = Path(datafile_path).parent.name\n",
    "        print(f\"Requesting config for {datafile_path}, searching key {data_folder}\")\n",
    "        return self.configs[data_folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class UnipenHandlerBuilder():\n",
    "    \"\"\"This class holds the knowledge of the UNIPEN data providers and their different formats.\n",
    "    It can be used to build the different online signals through builders adapted to providers.\"\"\"\n",
    "    unipen_root: str\n",
    "\n",
    "    provider_type_mapping = {\n",
    "        # SingleDocFileHandler: [\"abm\", \"anj\", \"apa\", \"apb\", \"apc\", \"apd\", \"ape\", \"app\", \"att\", \n",
    "        #                        \"bba\", \"bbb\", \"bbc\", \"bbd\", \n",
    "        #                        \"cea\", \"ceb\", \"cec\", \"ced\", \"cee\",\n",
    "        #                        \"dar\", \"gmd\", \"imt\", \"int\", \n",
    "        #                        \"lex\", \"par\", \"pcl\", \"pri\", \n",
    "        #                        \"rim\", \"scr\", \"uqb\"\n",
    "        #                        ],\n",
    "        MultiDocFileHandler: [\"hpb\", \"hpp\", \"huj\", \n",
    "                              \"kai\", \"kar\", \"lav\", \"lou\", \n",
    "                              \"mot\", \"pap\", \"phi\", \"sta\", \"syn\", \"val\"\n",
    "                              ],\n",
    "        # NoDoc: art, aga, atu, nic, sie (Special)\n",
    "        # Multi: tos(special), ugi(special)\n",
    "        # Problems\n",
    "        #   Empty: ata, cef, ibm, imp\n",
    "        #   No temporal info: not\n",
    "\n",
    "        #Par: .inc file changed to .doc and COORD added to it, as every stroke file has the same COORD system. Avoids creating a single handler for this one.\n",
    "        #PCL: Rename internal_pad.doc to .doc.other as it has the same information as the pcl.doc file.\n",
    "        #phi: Renaned *file* to *file*.doc\n",
    "        #HPP: rename hpp doc files form hpb* into hpp*\n",
    "        #HUJ: Put manudo datafile into huj8/ folder in order to have corresponding doc file\n",
    "        #STA: rename hpb{0,1}.doc to sta{0,1}.doc\n",
    "        #Val: Separate two writers into val01, val02 folders to correspond to expected structure\n",
    "    }\n",
    "\n",
    "    def __init__(self, unipen_root: str) -> None:\n",
    "        self.unipen_root = unipen_root\n",
    "\n",
    "    def build_handlers(self) -> list[UnipenHandler]:\n",
    "        \"\"\"Build all the handlers declared by this provider maping\"\"\"\n",
    "        provider_return: list[UnipenHandler] = []\n",
    "        \n",
    "        for handler_class, providers in self.provider_type_mapping.items():\n",
    "            print(f\"Len of providers: {len(providers)}\")\n",
    "            for provider in providers:\n",
    "                try:\n",
    "                    print(f\"Building provider {provider} as {handler_class.__name__}\")\n",
    "                    provider_path = os.path.join(self.unipen_root, provider)\n",
    "                    provider_handler = handler_class(provider_path)\n",
    "                    provider_return.append(provider_handler)\n",
    "                except Exception as e:\n",
    "                    print(f\"Impossible to build provider {provider} due to error: {e}\")\n",
    "                    raise e\n",
    "\n",
    "        return provider_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of providers: 13\n",
      "Building provider hpb as MultiDocFileHandler\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/hpb/hpb0.doc\n",
      "Create config hpb0\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/hpb/hpb1.doc\n",
      "Create config hpb1\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/hpb/hpb2.doc\n",
      "Create config hpb2\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/hpb/hpb3.doc\n",
      "Create config hpb3\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/hpb/hpb4.doc\n",
      "Create config hpb4\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/hpb/hpb5.doc\n",
      "Create config hpb5\n",
      "Building provider hpp as MultiDocFileHandler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/hpp/hpp0.doc\n",
      "Create config hpp0\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/hpp/hpp1.doc\n",
      "Create config hpp1\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/hpp/hpp2.doc\n",
      "Create config hpp2\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/hpp/hpp3.doc\n",
      "Create config hpp3\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/hpp/hpp4.doc\n",
      "Create config hpp4\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/hpp/phpb5.doc\n",
      "Create config phpb5\n",
      "Building provider huj as MultiDocFileHandler\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/huj/huj0.doc\n",
      "Create config huj0\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/huj/huj1.doc\n",
      "Create config huj1\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/huj/huj2.doc\n",
      "Create config huj2\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/huj/huj3.doc\n",
      "Create config huj3\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/huj/huj4.doc\n",
      "Create config huj4\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/huj/huj5.doc\n",
      "Create config huj5\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/huj/huj6.doc\n",
      "Create config huj6\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/huj/huj7.doc\n",
      "Create config huj7\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/huj/huj8.doc\n",
      "Create config huj8\n",
      "Building provider kai as MultiDocFileHandler\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/kai/kai0.doc\n",
      "Create config kai0\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/kai/kai1.doc\n",
      "Create config kai1\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/kai/kai2.doc\n",
      "Create config kai2\n",
      "Building provider kar as MultiDocFileHandler\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/kar/kar0.doc\n",
      "Create config kar0\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/kar/kar1.doc\n",
      "Create config kar1\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/kar/kar2.doc\n",
      "Create config kar2\n",
      "Building provider lav as MultiDocFileHandler\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/lav/lav0.doc\n",
      "Create config lav0\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/lav/lav1.doc\n",
      "Create config lav1\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/lav/lav2.doc\n",
      "Create config lav2\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/lav/lav3.doc\n",
      "Create config lav3\n",
      "Building provider lou as MultiDocFileHandler\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/lou/lou0.doc\n",
      "Create config lou0\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/lou/lou1.doc\n",
      "Create config lou1\n",
      "Building provider mot as MultiDocFileHandler\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/mot/mot1.doc\n",
      "Create config mot1\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/mot/mot2.doc\n",
      "Create config mot2\n",
      "Building provider pap as MultiDocFileHandler\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/pap/pap0.doc\n",
      "Create config pap0\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/pap/pap1.doc\n",
      "Create config pap1\n",
      "Building provider phi as MultiDocFileHandler\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_MM', 'Y_POINTS_PER_MM', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/phi/header_phi0.doc\n",
      "Create config phi0\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_MM', 'Y_POINTS_PER_MM', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/phi/header_phi1.doc\n",
      "Create config phi1\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_MM', 'Y_POINTS_PER_MM', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/phi/header_phi2.doc\n",
      "Create config phi2\n",
      "Building provider sta as MultiDocFileHandler\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/sta/hpb2.doc\n",
      "Create config hpb2\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/sta/hpb3.doc\n",
      "Create config hpb3\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/sta/hpb4.doc\n",
      "Create config hpb4\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/sta/hpb5.doc\n",
      "Create config hpb5\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/sta/sta0.doc\n",
      "Create config sta0\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/sta/sta1.doc\n",
      "Create config sta1\n",
      "Building provider syn as MultiDocFileHandler\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/syn/syn0.doc\n",
      "Create config syn0\n",
      "Missing key values {'Y_POINTS_PER_INCH', 'X_POINTS_PER_INCH'} on file ../../data/handwriting/Unipen/train_r01_v07/include/syn/syn1.doc\n",
      "Create config syn1\n",
      "Building provider val as MultiDocFileHandler\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/val/val01.doc\n",
      "Create config val01\n",
      "Missing key values {'X_POINTS_PER_MM', 'Y_POINTS_PER_MM'} on file ../../data/handwriting/Unipen/train_r01_v07/include/val/val02.doc\n",
      "Create config val02\n",
      "built 13 handlers\n"
     ]
    }
   ],
   "source": [
    "unipen_root = \"../../data/handwriting/Unipen/train_r01_v07/include\"\n",
    "\n",
    "unipen_handler_builder = UnipenHandlerBuilder(unipen_root)\n",
    "handlers = unipen_handler_builder.build_handlers()\n",
    "\n",
    "print(f\"built {len(handlers)} handlers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data\n",
      "Scanning ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb41afe.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb41ajj.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb41ic.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb41rjb.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb42afe.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb42ajj.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb42ic.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb42rjb.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb43afe.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb43ajj.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb43ic.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb43rjb.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb45afe.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb45ajj.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb45ic.dat, searching key hpb4\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb4/hpb45rjb.dat, searching key hpb4\n",
      "Scanning ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb5\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb5/hpb5-ang.dat, searching key hpb5\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb5/hpb5-das.dat, searching key hpb5\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb5/hpb5-jcb.dat, searching key hpb5\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb5/hpb5-ngb.dat, searching key hpb5\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb5/hpb5-scr.dat, searching key hpb5\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb5/hpb5-spc.dat, searching key hpb5\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpb/data/hpb5/hpb5-swd.dat, searching key hpb5\n",
      "strokes: 922\n",
      "Scanning ../../data/handwriting/Unipen/train_r01_v07/include/hpp/data\n",
      "Scanning ../../data/handwriting/Unipen/train_r01_v07/include/hpp/data/hpp2\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpp/data/hpp2/hpb2-an.dat, searching key hpp2\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpp/data/hpp2/hpb2-ar.dat, searching key hpp2\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpp/data/hpp2/hpb2-au.dat, searching key hpp2\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpp/data/hpp2/hpb2-ba.dat, searching key hpp2\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpp/data/hpp2/hpb2-bi.dat, searching key hpp2\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpp/data/hpp2/hpb2-bo.dat, searching key hpp2\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpp/data/hpp2/hpb2-br.dat, searching key hpp2\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpp/data/hpp2/hpb2-bt.dat, searching key hpp2\n",
      "Requesting config for ../../data/handwriting/Unipen/train_r01_v07/include/hpp/data/hpp2/hpb2-ch.dat, searching key hpp2\n"
     ]
    }
   ],
   "source": [
    "for handler in handlers:\n",
    "    handler.create_strokes()\n",
    "    print(f\"strokes: {len(handler.strokes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DRAW_COLOR_BLACK = 255\n",
    "DRAW_COLOR_WHITE = 0\n",
    "\n",
    "def create_image(signal: list[int, int, bool], draw_color = 1):\n",
    "    \"\"\"Create the image associated with the given signal.\"\"\"\n",
    "    max_h =  int(math.ceil(max(signal[:, 0])))\n",
    "    max_w = int(math.ceil(max(signal[:, 1])))\n",
    "\n",
    "    canvas = np.ascontiguousarray(np.full((max_w + 2, max_h + 2), DRAW_COLOR_BLACK), dtype=np.uint8)\n",
    "    print(f\"Canvas: {canvas.shape}\")\n",
    "    \n",
    "    #Draw lines from point (t-1) to current point (t) IFF the pen was not up. start with penup\n",
    "    #as we start from point 0.\n",
    "    draw_current_stroke = False\n",
    "    for x, y, eos in signal:\n",
    "        if draw_current_stroke:\n",
    "            cv2.line(canvas, (last_x, last_y), (x, y), DRAW_COLOR_WHITE, draw_color) \n",
    "        last_x, last_y, draw_current_stroke = x, y, not eos\n",
    "    \n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reasonable_stroke_size(stroke: np.ndarray):\n",
    "    return max(int(max(stroke[:, 1]) / 100), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "for i, handler in enumerate(handlers):\n",
    "    print(f\"Handler {i}: {handler.handler_root}\")\n",
    "    strokes = handler.strokes\n",
    "\n",
    "    n = randint(0, len(strokes) - 3)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3)\n",
    "\n",
    "    for i, stroke in enumerate(strokes[n:n+3]):\n",
    "        print(stroke.shape)\n",
    "        \n",
    "        image = create_image(stroke, compute_reasonable_stroke_size(stroke))\n",
    "\n",
    "        axs[i].imshow(image, cmap='gray', vmin=0, vmax=255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing problematic providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unipen_root = \"../../data/handwriting/Unipen/train_r01_v07/include\"\n",
    "provider = \"ced\"\n",
    "\n",
    "handler = SingleDocFileHandler(os.path.join(unipen_root, provider))\n",
    "handler.create_strokes()\n",
    "print(f\"Strokes: {len(handler.strokes)}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "strokes = handler.strokes\n",
    "\n",
    "n = randint(0, len(strokes) - 4)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "\n",
    "for i, stroke in enumerate(strokes[n:n+3]):\n",
    "    print(stroke.shape)\n",
    "    image = create_image(stroke, compute_reasonable_stroke_size(stroke))\n",
    "\n",
    "    axs[i].imshow(image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
